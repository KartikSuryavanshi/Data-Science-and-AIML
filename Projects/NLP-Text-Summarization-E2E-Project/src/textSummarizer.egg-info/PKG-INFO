Metadata-Version: 2.4
Name: textSummarizer
Version: 0.0.0
Summary: A small python package for NLP app
Home-page: https://github.com/entbappy/Text-Summarizer-Project
Author: entbappy
Author-email: entbappy73@gmail.com
Project-URL: Bug Tracker, https://github.com/entbappy/Text-Summarizer-Project/issues
License-File: LICENSE
Dynamic: author
Dynamic: author-email
Dynamic: description
Dynamic: home-page
Dynamic: license-file
Dynamic: project-url
Dynamic: summary

# ğŸ¤– End-to-End NLP Text Summarization Project

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.68+-green.svg)](https://fastapi.tiangolo.com/)
[![Transformers](https://img.shields.io/badge/ğŸ¤—_Transformers-4.0+-orange.svg)](https://huggingface.co/transformers/)

## ğŸ“– Project Overview

An **end-to-end production-ready text summarization system** that uses Google's **Pegasus transformer model** to generate concise summaries from long dialogues and conversations. The project includes complete ML pipeline implementation, from data ingestion to model deployment via REST API.

### ğŸ¯ Key Features

- Automated data pipeline (download, validation, transformation)
- Fine-tuned **Google Pegasus** model on SAMSum dataset
- FastAPI web server for real-time predictions
- Modular, production-grade code architecture
- Configuration-driven design (YAML configs)
- Comprehensive logging and error handling
- Docker support for containerization
- AWS deployment ready with CI/CD

---

## ğŸ—ï¸ Project Architecture

### ML Pipeline (5 Stages)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Data Ingestion   â”‚  â†’ Download SAMSum dataset from GitHub
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. Data Validation  â”‚  â†’ Verify train/test/validation files exist
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. Data Transform   â”‚  â†’ Tokenize dialogues & summaries
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. Model Training   â”‚  â†’ Fine-tune Pegasus on SAMSum
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. Model Evaluation â”‚  â†’ Calculate ROUGE metrics
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Technology Stack

- **Framework:** Hugging Face Transformers
- **Model:** Google Pegasus (pegasus-cnn_dailymail)
- **Dataset:** SAMSum (16k+ conversations with summaries)
- **API:** FastAPI + Uvicorn
- **Deep Learning:** PyTorch
- **Deployment:** Docker + AWS EC2/ECR

---

## ğŸš€ Quick Start

### Prerequisites

- Python 3.8+
- pip or conda
- (Optional) CUDA-enabled GPU for faster training

### Installation

Clone the repository

```bash
git clone https://github.com/KartikSuryavanshi/NLP-Text-Summarization.git
cd NLP-Text-Summarization
```
**Option 1: Using Conda** (Recommended)

```bash
# Create and activate conda environment
conda create -n summary python=3.8 -y
conda activate summary

# Install dependencies
pip install -r requirements.txt

# Install the textSummarizer package
pip install -e .
```

**Option 2: Using venv**

```bash
# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
pip install -e .
```

---

## ğŸ’» Usage

### 1ï¸âƒ£ Train the Model (Optional)

If you want to retrain the model from scratch:

```bash
python main.py
```

This will execute all 5 pipeline stages sequentially:
- Downloads SAMSum dataset (~10MB)
- Validates data files
- Tokenizes conversations
- Fine-tunes Pegasus model (takes ~2-4 hours on GPU)
- Evaluates model performance

### 2ï¸âƒ£ Start the Web Server

```bash
python app.py
```

Server will start at: **http://localhost:8080**

### 3ï¸âƒ£ Use the API

**Option A: Interactive Swagger UI**

1. Open http://localhost:8080/docs in your browser
2. Navigate to `/predict` endpoint
3. Click "Try it out"
4. Paste your text and click "Execute"

**Option B: cURL Command**

```bash
curl -X POST "http://localhost:8080/predict" \
  -H "Content-Type: application/json" \
  -d '{"text": "Your long conversation or dialogue here..."}'
```

**Option C: Python Requests**

```python
import requests

text = """
John: Hey Sarah, did you finish the quarterly report?
Sarah: Yes, I submitted it yesterday. It includes all the sales metrics.
John: Great! What were the main findings?
Sarah: Sales increased by 23% compared to last quarter.
"""

response = requests.post(
    "http://localhost:8080/predict",
    json={"text": text}
)
print(response.json())
```

---

## ğŸ“‚ Project Structure

```
â”œâ”€â”€ artifacts/                  # ML outputs (auto-generated)
â”‚   â”œâ”€â”€ data_ingestion/        # Downloaded dataset
â”‚   â”œâ”€â”€ data_transformation/   # Tokenized data
â”‚   â”œâ”€â”€ model_trainer/         # Trained model
â”‚   â””â”€â”€ model_evaluation/      # Performance metrics
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.yaml            # Pipeline configurations
â”‚
â”œâ”€â”€ src/textSummarizer/
â”‚   â”œâ”€â”€ conponents/            # Core ML components
â”‚   â”‚   â”œâ”€â”€ data_ingestion.py
â”‚   â”‚   â”œâ”€â”€ data_validation.py
â”‚   â”‚   â”œâ”€â”€ data_transformation.py
â”‚   â”‚   â”œâ”€â”€ model_trainer.py
â”‚   â”‚   â””â”€â”€ model_evaluation.py
â”‚   â”‚
â”‚   â”œâ”€â”€ pipeline/              # Training & prediction pipelines
â”‚   â”‚   â”œâ”€â”€ stage_01_data_ingestion.py
â”‚   â”‚   â”œâ”€â”€ ...
â”‚   â”‚   â””â”€â”€ prediction.py
â”‚   â”‚
â”‚   â”œâ”€â”€ config/                # Configuration management
â”‚   â”‚   â””â”€â”€ configuration.py
â”‚   â”‚
â”‚   â”œâ”€â”€ entity/                # Data classes
â”‚   â”œâ”€â”€ utils/                 # Helper functions
â”‚   â””â”€â”€ logging/               # Custom logging
â”‚
â”œâ”€â”€ research/                   # Jupyter notebooks for experiments
â”‚   â”œâ”€â”€ 01_data_ingestion.ipynb
â”‚   â”œâ”€â”€ 02_data_validation.ipynb
â”‚   â”œâ”€â”€ 03_data_transformation.ipynb
â”‚   â”œâ”€â”€ 04_model_trainer.ipynb
â”‚   â””â”€â”€ 05_Model_evaluation.ipynb
â”‚
â”œâ”€â”€ app.py                      # FastAPI web server
â”œâ”€â”€ main.py                     # Training pipeline orchestrator
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ setup.py                    # Package setup
â”œâ”€â”€ params.yaml                 # Training hyperparameters
â””â”€â”€ Dockerfile                  # Docker configuration
```

---

## ğŸ”§ Configuration Files

### config.yaml
Defines paths and model settings for each pipeline stage.

```yaml
artifacts_root: artifacts

data_ingestion:
  source_URL: https://github.com/entbappy/Branching-tutorial/raw/master/summarizer-data.zip
  
model_trainer:
  model_ckpt: google/pegasus-cnn_dailymail
```

### params.yaml
Training hyperparameters.

```yaml
TrainingArguments:
  num_train_epochs: 1
  per_device_train_batch_size: 1
  warmup_steps: 500
```

---

## ğŸ“Š Model Details

**Model:** Google Pegasus (Pre-trained on CNN/DailyMail)

**Fine-tuning Dataset:** SAMSum
- 16,369 conversations with human-written summaries
- Messenger-like conversations
- Average dialogue length: 94 words
- Average summary length: 20 words

**Training Configuration:**
- Epochs: 1 (for quick training)
- Batch Size: 1
- Max Input Length: 1024 tokens
- Max Output Length: 128 tokens
- Beam Search: 8 beams
- Length Penalty: 0.8

---

## ğŸ§ª API Endpoints

| Method | Endpoint   | Description                      |
|--------|-----------|----------------------------------|
| GET    | `/`       | Redirects to API docs            |
| GET    | `/docs`   | Interactive API documentation    |
| GET    | `/train`  | Trigger model retraining         |
| POST   | `/predict`| Generate text summary            |

---

## ğŸ³ Docker Deployment

### Build and run with Docker

```bash
# Build the Docker image
docker build -t text-summarizer .

# Run the container
docker run -p 8080:8080 text-summarizer
```

Access the application at http://localhost:8080

---

## â˜ï¸ AWS Deployment with CI/CD

### Prerequisites

1. **AWS Account** with access to EC2 and ECR
2. **GitHub Account** for CI/CD automation

### Deployment Steps

#### 1. Create IAM User

Create IAM user with the following policies:
- `AmazonEC2ContainerRegistryFullAccess`
- `AmazonEC2FullAccess`

Save the **Access Key ID** and **Secret Access Key**

#### 2. Create ECR Repository

```bash
aws ecr create-repository --repository-name text-summarizer --region us-east-1
```

Save the repository URI (e.g., `566373416292.dkr.ecr.us-east-1.amazonaws.com/text-summarizer`)

#### 3. Launch EC2 Instance

- Choose **Ubuntu 22.04 LTS**
- Instance Type: **t2.medium** or higher (for model inference)
- Security Group: Allow inbound traffic on port **8080**

#### 4. Install Docker on EC2

SSH into your EC2 instance and run:

```bash
# Update system
sudo apt-get update -y
sudo apt-get upgrade -y

# Install Docker
curl -fsSL https://get.docker.com -o get-docker.sh
sudo sh get-docker.sh

# Add ubuntu user to docker group
sudo usermod -aG docker ubuntu
newgrp docker
```

#### 5. Configure GitHub Secrets

Go to your repository: **Settings â†’ Secrets and variables â†’ Actions**

Add the following secrets:

```
AWS_ACCESS_KEY_ID=<your-access-key>
AWS_SECRET_ACCESS_KEY=<your-secret-key>
AWS_REGION=us-east-1
AWS_ECR_LOGIN_URI=566373416292.dkr.ecr.us-east-1.amazonaws.com
ECR_REPOSITORY_NAME=text-summarizer
```

#### 6. Setup Self-Hosted Runner

On your EC2 instance:
1. Go to: **GitHub Repo â†’ Settings â†’ Actions â†’ Runners â†’ New self-hosted runner**
2. Choose **Linux** as OS
3. Run the provided commands on your EC2 instance

#### 7. Push to Deploy

Once configured, every push to the `main` branch will:
1. Build Docker image
2. Push to AWS ECR
3. Deploy to EC2 automatically

---

## ğŸ”¬ Research Notebooks

Explore the development process through Jupyter notebooks:

1. **[01_data_ingestion.ipynb](research/01_data_ingestion.ipynb)** - Dataset download and extraction
2. **[02_data_validation.ipynb](research/02_data_validation.ipynb)** - Data integrity checks
3. **[03_data_transformation.ipynb](research/03_data_transformation.ipynb)** - Tokenization pipeline
4. **[04_model_trainer.ipynb](research/04_model_trainer.ipynb)** - Model fine-tuning
5. **[05_Model_evaluation.ipynb](research/05_Model_evaluation.ipynb)** - Performance metrics

---

## ğŸ§° Development Workflow

When adding new features, follow this sequence:

1. Update `config/config.yaml` (if new configs needed)
2. Update `params.yaml` (if new hyperparameters needed)
3. Update entity classes in `src/textSummarizer/entity/`
4. Update `ConfigurationManager` in `src/textSummarizer/config/configuration.py`
5. Implement component in `src/textSummarizer/conponents/`
6. Create pipeline stage in `src/textSummarizer/pipeline/`
7. Update `main.py` (for training) or `app.py` (for inference)
8. Test in research notebooks first

---

## ğŸ› Troubleshooting

### Issue: `ModuleNotFoundError: No module named 'textSummarizer'`

**Solution:**
```bash
pip install -e .
# Then restart your Jupyter kernel
```

### Issue: Out of memory during training

**Solution:**
- Reduce `per_device_train_batch_size` in `params.yaml`
- Increase `gradient_accumulation_steps`
- Use a smaller model variant

### Issue: Server not accessible

**Solution:**
```bash
# Check if server is running
ps aux | grep python

# Kill existing process
pkill -f "python app.py"

# Restart server
python app.py
```

---

## ğŸ“ˆ Performance Metrics

After training, check `artifacts/model_evaluation/metrics.csv` for:
- **ROUGE-1**: Unigram overlap
- **ROUGE-2**: Bigram overlap  
- **ROUGE-L**: Longest common subsequence

Typical fine-tuned Pegasus scores on SAMSum:
- ROUGE-1: ~42-45
- ROUGE-2: ~20-23
- ROUGE-L: ~33-36

---

## ğŸ¤ Contributing

Contributions are welcome! Please follow these steps:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

---

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ‘¨â€ğŸ’» Author

**Your Name**
- GitHub: [@yourusername](https://github.com/yourusername)
- Email: your.email@example.com

---

## ğŸ™ Acknowledgments

- **Google Research** for the Pegasus model
- **Hugging Face** for the Transformers library
- **SAMSum Dataset** creators for the conversation corpus
- **Krish Naik** for project inspiration

---

## ğŸ“š References

- [Pegasus Paper](https://arxiv.org/abs/1912.08777)
- [SAMSum Dataset](https://arxiv.org/abs/1911.12237)
- [Hugging Face Documentation](https://huggingface.co/docs)
- [FastAPI Documentation](https://fastapi.tiangolo.com/)

---

## ğŸ”® Future Enhancements

- [ ] Add support for multiple summarization models
- [ ] Implement caching for faster inference
- [ ] Add multilingual support
- [ ] Create web UI with React/Vue
- [ ] Add batch processing endpoint
- [ ] Implement model versioning
- [ ] Add monitoring with Prometheus/Grafana
- [ ] Support for custom fine-tuning datasets

---

**â­ If you found this project helpful, please give it a star!**
